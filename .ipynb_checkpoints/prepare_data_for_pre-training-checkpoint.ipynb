{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493b039e-a70f-42a9-84a7-1ad0b1c04722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3dbf19-5ecc-44b6-bcbb-a136aa52adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_old = datasets.load_from_disk('./formatted_data_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3e365-6bbf-4f31-81f2-6b77179f3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d8947-2e00-4ac7-8f5d-65139e6e5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"./../../../from_shala/vocabulary_analysis/data_filtration/Filtration_25Dec2022/ALL_FILTERED_DATA/processed_data.json\"\n",
    "\n",
    "with open(path_file, 'r') as f:\n",
    "    data_new = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b4bb2-9ef5-4534-96f0-e359c34cb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences(\n",
    "    dict_data,\n",
    "    max_len=128,\n",
    "    sentence_split_ratio=1.2,\n",
    "    key='TEXT'\n",
    "):\n",
    "    print('grouping sentences...')\n",
    "    list_out = []\n",
    "    cur_sequence = ''\n",
    "    idx = 0\n",
    "    for k_, v_ in tqdm(dict_data.items()):\n",
    "        idx += 1\n",
    "        new_sequence = cur_sequence + v_[key] + ' '\n",
    "        if (len(new_sequence.split(' ')) * sentence_split_ratio) >= max_len:\n",
    "            list_out.append(cur_sequence)\n",
    "            cur_sequence = v_[key] + ' '\n",
    "        else:\n",
    "            cur_sequence = new_sequence\n",
    "\n",
    "        #\n",
    "        #if idx > 10000:\n",
    "        #    break\n",
    "    print('done')\n",
    "\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791d12f-dbf8-4c6a-bec9-bfb1cb2787e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_, v_ in data_new.items():\n",
    "    print(k_)\n",
    "    print(v_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f06f7-e11c-4378-887a-320c5d65e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_list = group_sentences(data_new)\n",
    "flag_new = ['new'] * len(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097aee29-798e-4455-9585-9a90a952c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c1da3-8d57-48be-a5b0-930f38fabd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_old_list = list(data_old['train']['TEXT']) + list(data_old['test']['TEXT']) + list(data_old['validation']['TEXT'])\n",
    "flag_old = ['old'] * len(data_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6f6e0-eb30-4904-aea4-bdb6a24e7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_old_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bed755-926b-4090-99dd-0f1f358dd377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(data_old_list)\n",
    "random.shuffle(data_new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7eafa9-d2ec-421f-b844-59fc2092311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_old_list[0])\n",
    "print(\"==\"*10)\n",
    "print(data_new_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8964e5-9138-4f64-9589-a36b878db42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "train = data_old_list[:-150000]\n",
    "validation = data_old_list[-150000:-75000]\n",
    "test = data_old_list[-75000:]\n",
    "\n",
    "#\n",
    "train += data_new_list[:-50000]\n",
    "validation += data_new_list[-50000:-25000]\n",
    "test += data_new_list[-25000:]\n",
    "\n",
    "#\n",
    "random.shuffle(train)\n",
    "random.shuffle(validation)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f5741-0179-422c-8d00-1b986ab70ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(validation))\n",
    "print(len(test))\n",
    "\n",
    "#print(len(flag_train))\n",
    "#print(len(flag_val))\n",
    "#print(len(flag_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b871a2-412e-4632-9a3d-fd8bad2c1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    \"TEXT\": train,\n",
    "    #\"FLAG\": flag_train,\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"TEXT\": validation,\n",
    "    #\"FLAG\": flag_val,\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"TEXT\": test,\n",
    "    #\"FLAG\": flag_test,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff29606-8306-4bbf-995e-e8d1c025f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hf = datasets.DatasetDict()\n",
    "\n",
    "data_hf['train'] = datasets.Dataset.from_pandas(train_df)\n",
    "data_hf['validation'] = datasets.Dataset.from_pandas(val_df)\n",
    "data_hf['test'] = datasets.Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468540be-3265-4050-93fd-d1f0966dfd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbb561-3f59-4502-9e2b-8a67ee70e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hf.save_to_disk('./pretraining_data_01Jan2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd26e0f-f173-45a7-8aeb-b3e9c2239bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read = datasets.load_from_disk('./pretraining_data_01Jan2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e19b50-6689-4f7b-8044-ba6634a15aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = data_read['train']['TEXT'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc7a3e-8f7c-4bf5-8f8b-8d1f8e59f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bf926d-c0f5-428e-b66b-42c7e34e3284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "752320it [02:17, 5456.74it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3929619/2083657290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare data without vocab constraint\n",
    "dataset_ = datasets.load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "dataset_ = dataset_.shuffle(seed=0)\n",
    "\n",
    "#\n",
    "span_length=110\n",
    "stride=30\n",
    "\n",
    "#\n",
    "list_data = []\n",
    "for example in tqdm(dataset_):\n",
    "    doc = example[\"text\"]\n",
    "    words = doc.split(' ')\n",
    "    for start in range(0, len(words), stride):\n",
    "        end = min(start+span_length, len(words))\n",
    "        span = ' '.join(words[start:end])\n",
    "        list_data.append(span)\n",
    "    \n",
    "    #\n",
    "    if len(list_data) >= (9281490):\n",
    "        print(len(list_data))\n",
    "        print(list_data[0])\n",
    "        break\n",
    "\n",
    "#\n",
    "random.shuffle(list_data)\n",
    "train = list_data[:-200000]\n",
    "validation = list_data[-200000:-100000]\n",
    "test = list_data[-100000:]\n",
    "\n",
    "#\n",
    "train_df = pd.DataFrame({\n",
    "    \"TEXT\": train,\n",
    "    #\"FLAG\": flag_train,\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"TEXT\": validation,\n",
    "    #\"FLAG\": flag_val,\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"TEXT\": test,\n",
    "    #\"FLAG\": flag_test,\n",
    "})\n",
    "\n",
    "#\n",
    "data_hf = datasets.DatasetDict()\n",
    "data_hf['train'] = datasets.Dataset.from_pandas(train_df)\n",
    "data_hf['validation'] = datasets.Dataset.from_pandas(val_df)\n",
    "data_hf['test'] = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "#\n",
    "data_hf.save_to_disk('./pretraining_data_free_text_08Jan2022')\n",
    "data_read = datasets.load_from_disk('./pretraining_data_free_text_08Jan2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1ac46-563f-458c-90bb-1f0c8850ab2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
