{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deac79cc-51dc-4afe-b33b-ab12b45cf6f8",
   "metadata": {},
   "source": [
    "# Building vocabulary from Wikipedia articles\n",
    "\n",
    "Pre-processing steps we take:\n",
    "1. All text is lowercases\n",
    "2. We replace '-' with space because '-' containing words most likely are composed of words already present in the vocabulary\n",
    "3. We repalce numbers and digits with '#NUMBER' to avoid unneccessary increase in the vocabulary size\n",
    "4. We replace the hyperlinks by '#HLINK' because for our our study purposes hyperlinks are not useful\n",
    "\n",
    "Building vocabulary:\n",
    "1. Initially we were using NLTK's vocabulary for English that contains approximately 230k words. While working with the Wikipedia articles we realized that this 230k vocabulary is not enough. To give an example, 'marxism' word was not present in the NLTK's vocabulary. Now, we may think that if we are filtering sentences based on words spoken by siz year olds then 'marxism', in particular, might not be that important. But, we don't know what is missing in the NLTK's vocabulary and we don't want to miss out on selecting a simple sentence because the word was not present in NLTK.\n",
    "2. We first sequentially go through wikipedia articles building the vocabulary, until the vocabulary size reaches 2 million. There are many information resources that indicate actual vocabulary size of English is much smaller. But, 2 million is just to be safe. It required 260k articles to develop a vocabulary of 2 million.\n",
    "3. We check all words from NLTK and all words from AOCHildes are present in the 2 million\n",
    "4. We sample 260k documents from previously unseen document, build new vocabulary and merge with the existing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cf2cc8-b56d-468c-bd60-ed03e54563ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "#from english_words import english_words_set\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zipf\n",
    "from scipy.optimize import curve_fit\n",
    "from nltk.corpus import words\n",
    "import vocab_utils as utils_\n",
    "from nltk import FreqDist, word_tokenize, wordpunct_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d5583a-6831-4cf9-8e93-660608bf0aeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - '/home/vdeshpande/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/share/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words.zip/words/\u001b[0m\n\n  Searched in:\n    - '/home/vdeshpande/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/share/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_742623/1998090078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maoc_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CHILDES_vocab_age.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - '/home/vdeshpande/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/share/nltk_data'\n    - '/home/vdeshpande/miniconda3/envs/general/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "nltk_vocab = words.words()\n",
    "aoc_vocab = pd.read_csv('CHILDES_vocab_age.csv').loc[:, 'word'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b8d09b-8193-4d68-867a-71652e293d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexps for text preprocessing\n",
    "ONLY_ALPHA = re.compile(r'([^\\s\\w]|_)+')\n",
    "NUMBERS = re.compile(r'\\b\\d+')\n",
    "MULTISPACE = re.compile(r'[^\\S\\r\\n]{2,}')\n",
    "AT_DIGIT = re.compile(r'@[,.]@')\n",
    "AT_HYPHEN = re.compile(r'@-@')\n",
    "WORD = re.compile(r'\\w+')\n",
    "HLINK = re.compile(r'http\\S+')\n",
    "\n",
    "\n",
    "def preprocess(line):\n",
    "    #line = line.replace('``', '\"')\n",
    "    #line = line.replace(\"''\", '\"')\n",
    "    #line = AT_HYPHEN.sub('-', line)\n",
    "    line = line.replace('-', ' ')\n",
    "    line = AT_DIGIT.sub('#NUMBER', line)\n",
    "    line = NUMBERS.sub('#NUMBER ', line)\n",
    "    line = HLINK.sub('#HLINK', line)\n",
    "    #line = MULTISPACE.sub(' ', line)\n",
    "    #line = line.lstrip(' ')\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5ce7c5-b6d4-40d1-8f09-2c0368af2588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20220301.en to /home/hf_cache/datasets_cache/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15.3k/15.3k [00:00<00:00, 6.31MB/s]\n"
     ]
    },
    {
     "ename": "MissingBeamOptions",
     "evalue": "Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \nExample of usage: \n\t`load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingBeamOptions\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_742623/3522340252.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwiki_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wikipedia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"20220301.en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwiki_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwiki_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wikipedia_vocab_regex_based.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         \u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m         \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m     )\n\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m                             \u001b[0mverify_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                             \u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                         )\n\u001b[1;32m    828\u001b[0m                     \u001b[0;31m# Sync info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/general/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m             \u001b[0musage_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"load_dataset('{self.name}', '{self.config.name}', beam_runner='DirectRunner')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             raise MissingBeamOptions(\n\u001b[0;32m-> 1880\u001b[0;31m                 \u001b[0;34m\"Trying to generate a dataset using Apache Beam, yet no Beam Runner \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0;34m\"or PipelineOptions() has been provided in `load_dataset` or in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0;34m\"builder arguments. For big datasets it has to run on large-scale data \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingBeamOptions\u001b[0m: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \nExample of usage: \n\t`load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')`"
     ]
    }
   ],
   "source": [
    "wiki_data = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "wiki_docs = wiki_data['train']['text']\n",
    "wiki_vocab = pd.read_csv('wikipedia_vocab_regex_based.csv').iloc[:, 1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d7c57-2820-46d9-a1f7-bd2bbc76e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = [] + wiki_vocab\n",
    "article_idx = 0\n",
    "for article in tqdm(wiki_docs):\n",
    "    article_idx += 1\n",
    "    \"\"\"\n",
    "    if article_idx < 250000:\n",
    "        continue\n",
    "    \"\"\"\n",
    "    article = preprocess(article)\n",
    "    english_vocab += WORD.findall(article.lower())#word_tokenize(article.lower())\n",
    "        \n",
    "    #\n",
    "    if article_idx%10000 == 0:\n",
    "        english_vocab = list(set(english_vocab))\n",
    "        pd.DataFrame(english_vocab).to_csv('wikipedia_vocab_regex_based.csv')\n",
    "        \n",
    "        if len(english_vocab) >= 2e6:\n",
    "            break\n",
    "            \n",
    "\n",
    "print(len(set(english_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09e99e-2915-4eab-b1c8-90d878664dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = [str(i) for i in english_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386a4ad-af30-42ef-8662-eef1ec592720",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_nltk = set(nltk_vocab) - set(english_vocab)\n",
    "missing_aoc = set(aoc_vocab) - set(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293be25-19a3-4c4c-862b-fab2e5c08f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*10)\n",
    "print(f'number of words in NLTK but not in Wiki: {len(missing_nltk)}')\n",
    "#print(missing_nltk)\n",
    "\n",
    "print('='*10)\n",
    "print(f'number of words in AOC but not in Wiki: {len(missing_aoc)}')\n",
    "#print(missing_aoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b970fc-f8ee-4fae-95c9-1c8c765aaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab += list(nltk_vocab)\n",
    "english_vocab += list(aoc_vocab)\n",
    "english_vocab = list(set(english_vocab))\n",
    "pd.DataFrame(english_vocab).to_csv('wikipedia_vocab_regex_based.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289f9b0-2dd4-4360-832d-0e53c5433617",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_articles = np.random.choice([i for i in range(260000, len(wiki_data['train']))], size=260000, replace=False)\n",
    "new_articles = [wiki_docs[i] for i in random_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f978d4-0867-4302-a852-8ca5299d357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_vocab = []\n",
    "new_vocab = []\n",
    "article_idx = 0\n",
    "for article in tqdm(new_articles):\n",
    "    article_idx += 1\n",
    "    article = preprocess(article)\n",
    "    new_vocab += WORD.findall(article.lower())#word_tokenize(article.lower())\n",
    "        \n",
    "    #\n",
    "    if article_idx%50000 == 0:\n",
    "        new_vocab = list(set(new_vocab))\n",
    "        not_in_vocab += list(set(new_vocab) - set(english_vocab))\n",
    "        not_in_vocab = list(set(not_in_vocab))\n",
    "        print(f'There are {len(not_in_vocab)} number of words found that are not present in the vocabulary')\n",
    "        \n",
    "        if len(new_vocab) >= 2e6:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4cb8f-877e-482f-86c4-1f8f41316df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2c17d-0994-4cee-87a4-88577bf6b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both vocab\n",
    "english_vocab += list(new_vocab)\n",
    "english_vocab = list(set(english_vocab))\n",
    "pd.DataFrame(english_vocab).to_csv('wikipedia_vocab_regex_based.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68baaa85-4472-4d1c-a202-02dd1574ca32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
