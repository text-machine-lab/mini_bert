{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deac79cc-51dc-4afe-b33b-ab12b45cf6f8",
   "metadata": {},
   "source": [
    "# Building vocabulary from Wikipedia articles\n",
    "\n",
    "Pre-processing steps we take:\n",
    "1. All text is lowercases\n",
    "2. We replace '-' with space because '-' containing words most likely are composed of words already present in the vocabulary\n",
    "3. We repalce numbers and digits with '#NUMBER' to avoid unneccessary increase in the vocabulary size\n",
    "4. We replace the hyperlinks by '#HLINK' because for our our study purposes hyperlinks are not useful\n",
    "\n",
    "Building vocabulary:\n",
    "1. Initially we were using NLTK's vocabulary for English that contains approximately 230k words. While working with the Wikipedia articles we realized that this 230k vocabulary is not enough. To give an example, 'marxism' word was not present in the NLTK's vocabulary. Now, we may think that if we are filtering sentences based on words spoken by siz year olds then 'marxism', in particular, might not be that important. But, we don't know what is missing in the NLTK's vocabulary and we don't want to miss out on selecting a simple sentence because the word was not present in NLTK.\n",
    "2. We first sequentially go through wikipedia articles building the vocabulary, until the vocabulary size reaches 2 million. There are many information resources that indicate actual vocabulary size of English is much smaller. But, 2 million is just to be safe. It required 260k articles to develop a vocabulary of 2 million.\n",
    "3. We check all words from NLTK and all words from AOCHildes are present in the 2 million\n",
    "4. We sample 260k documents from previously unseen document, build new vocabulary and merge with the existing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3cf2cc8-b56d-468c-bd60-ed03e54563ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdeshpande/miniconda3/envs/vlad_work/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from english_words import english_words_set\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zipf\n",
    "from scipy.optimize import curve_fit\n",
    "from nltk.corpus import words\n",
    "import vocab_utils as utils_\n",
    "from nltk import FreqDist, word_tokenize, wordpunct_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d5583a-6831-4cf9-8e93-660608bf0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "nltk_vocab = words.words()\n",
    "aoc_vocab = pd.read_csv('CHILDES_vocab_age.csv').loc[:, 'word'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b8d09b-8193-4d68-867a-71652e293d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexps for text preprocessing\n",
    "ONLY_ALPHA = re.compile(r'([^\\s\\w]|_)+')\n",
    "NUMBERS = re.compile(r'\\b\\d+')\n",
    "MULTISPACE = re.compile(r'[^\\S\\r\\n]{2,}')\n",
    "AT_DIGIT = re.compile(r'@[,.]@')\n",
    "AT_HYPHEN = re.compile(r'@-@')\n",
    "WORD = re.compile(r'\\w+')\n",
    "HLINK = re.compile(r'http\\S+')\n",
    "\n",
    "\n",
    "def preprocess(line):\n",
    "    #line = line.replace('``', '\"')\n",
    "    #line = line.replace(\"''\", '\"')\n",
    "    #line = AT_HYPHEN.sub('-', line)\n",
    "    line = line.replace('-', ' ')\n",
    "    line = AT_DIGIT.sub('#NUMBER', line)\n",
    "    line = NUMBERS.sub('#NUMBER ', line)\n",
    "    line = HLINK.sub('#HLINK', line)\n",
    "    #line = MULTISPACE.sub(' ', line)\n",
    "    #line = line.lstrip(' ')\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5ce7c5-b6d4-40d1-8f09-2c0368af2588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/home/hf_cache/datasets_cache/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.03it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_data = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "wiki_docs = wiki_data['train']['text']\n",
    "wiki_vocab = pd.read_csv('wikipedia_vocab_regex_based.csv').iloc[:, 1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783d7c57-2820-46d9-a1f7-bd2bbc76e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▏                             | 249999/6458670 [00:03<01:19, 78425.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2180879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "english_vocab = [] + wiki_vocab\n",
    "article_idx = 0\n",
    "for article in tqdm(wiki_docs):\n",
    "    article_idx += 1\n",
    "    \"\"\"\n",
    "    if article_idx < 250000:\n",
    "        continue\n",
    "    \"\"\"\n",
    "    article = preprocess(article)\n",
    "    english_vocab += WORD.findall(article.lower())#word_tokenize(article.lower())\n",
    "        \n",
    "    #\n",
    "    if article_idx%10000 == 0:\n",
    "        english_vocab = list(set(english_vocab))\n",
    "        pd.DataFrame(english_vocab).to_csv('wikipedia_vocab_regex_based.csv')\n",
    "        \n",
    "        if len(english_vocab) >= 2e6:\n",
    "            break\n",
    "            \n",
    "\n",
    "print(len(set(english_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a09e99e-2915-4eab-b1c8-90d878664dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = [str(i) for i in english_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9386a4ad-af30-42ef-8662-eef1ec592720",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_nltk = set(nltk_vocab) - set(english_vocab)\n",
    "missing_aoc = set(aoc_vocab) - set(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6293be25-19a3-4c4c-862b-fab2e5c08f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "number of words in NLTK but not in Wiki: 1\n",
      "==========\n",
      "number of words in AOC but not in Wiki: 1\n"
     ]
    }
   ],
   "source": [
    "print('='*10)\n",
    "print(f'number of words in NLTK but not in Wiki: {len(missing_nltk)}')\n",
    "#print(missing_nltk)\n",
    "\n",
    "print('='*10)\n",
    "print(f'number of words in AOC but not in Wiki: {len(missing_aoc)}')\n",
    "#print(missing_aoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b970fc-f8ee-4fae-95c9-1c8c765aaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab += list(nltk_vocab)\n",
    "english_vocab += list(aoc_vocab)\n",
    "english_vocab = list(set(english_vocab))\n",
    "pd.DataFrame(english_vocab).to_csv('wikipedia_vocab_regex_based.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f289f9b0-2dd4-4360-832d-0e53c5433617",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_articles = np.random.choice([i for i in range(260000, len(wiki_data['train']))], size=260000, replace=False)\n",
    "new_articles = [wiki_docs[i] for i in random_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f978d4-0867-4302-a852-8ca5299d357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████▋                           | 50950/260000 [00:10<02:58, 1172.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 128548 number of words found that are not present in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████▊                    | 101038/260000 [00:21<02:25, 1095.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 253135 number of words found that are not present in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████▏             | 151139/260000 [00:31<01:40, 1078.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 371033 number of words found that are not present in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████▌       | 201272/260000 [00:41<00:54, 1069.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 481238 number of words found that are not present in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████▊ | 250904/260000 [00:52<00:09, 938.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 586063 number of words found that are not present in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 260000/260000 [00:53<00:00, 4816.95it/s]\n"
     ]
    }
   ],
   "source": [
    "not_in_vocab = []\n",
    "new_vocab = []\n",
    "article_idx = 0\n",
    "for article in tqdm(new_articles):\n",
    "    article_idx += 1\n",
    "    article = preprocess(article)\n",
    "    new_vocab += WORD.findall(article.lower())#word_tokenize(article.lower())\n",
    "        \n",
    "    #\n",
    "    if article_idx%50000 == 0:\n",
    "        new_vocab = list(set(new_vocab))\n",
    "        not_in_vocab += list(set(new_vocab) - set(english_vocab))\n",
    "        not_in_vocab = list(set(not_in_vocab))\n",
    "        print(f'There are {len(not_in_vocab)} number of words found that are not present in the vocabulary')\n",
    "        \n",
    "        if len(new_vocab) >= 2e6:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c4cb8f-877e-482f-86c4-1f8f41316df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5651913\n"
     ]
    }
   ],
   "source": [
    "print(len(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94c2c17d-0994-4cee-87a4-88577bf6b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both vocab\n",
    "english_vocab += list(new_vocab)\n",
    "english_vocab = list(set(english_vocab))\n",
    "pd.DataFrame(english_vocab).to_csv('wikipedia_vocab_regex_based.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68baaa85-4472-4d1c-a202-02dd1574ca32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
