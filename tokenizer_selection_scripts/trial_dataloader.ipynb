{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5008f6-4b5c-45d0-8855-73d2fdbe0fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdeshpande/miniconda3/envs/general/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForWholeWordMask\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef359af-d212-4e4e-a7cf-8f4624e7db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42056028-fdee-407d-b7cc-40c21becd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        list_data,\n",
    "        tokenizer,\n",
    "        max_seq_len,\n",
    "    ):\n",
    "        \n",
    "        #\n",
    "        self.data = list_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_seq_len\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def __len__(\n",
    "        self,    \n",
    "    ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx,\n",
    "    ):\n",
    "        #\n",
    "        example = self.data[idx]\n",
    "        \n",
    "        # tokenize\n",
    "        tokenized = self.tokenizer.encode_plus(\n",
    "            text=example,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "        )\n",
    "        \n",
    "        #\n",
    "        #tokenized['labels'] = tokenized['input_ids']\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "class LMDataloader():\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dict_data,\n",
    "        tokenizer,\n",
    "        mlm_probability,\n",
    "        max_seq_len,\n",
    "        batch_size=8,\n",
    "        validation_size=0.05,\n",
    "        fixed_seed_val=0,\n",
    "    ):\n",
    "        \n",
    "        # covert dictionary of sentences into a list of grouped sentences\n",
    "        list_data = self.group_sentences(\n",
    "            dict_data=dict_data,\n",
    "            max_len=max_seq_len,\n",
    "        )\n",
    "        \n",
    "        # split the data\n",
    "        train, val = self.split_data(\n",
    "            list_data=list_data,\n",
    "            validation_size=validation_size,\n",
    "            fixed_seed_val=fixed_seed_val,\n",
    "        )\n",
    "        \n",
    "        # convert into dataset format\n",
    "        self.dataset = {}\n",
    "        self.dataset['train'] = LMDataset(\n",
    "            list_data=train,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        self.dataset['validation'] = LMDataset(\n",
    "            list_data=val,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        \n",
    "        # define data collator\n",
    "        collator_obj = DataCollatorForWholeWordMask(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=True,\n",
    "            mlm_probability=mlm_probability,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        #\n",
    "        self.dataloader = {}\n",
    "        for split in self.dataset:\n",
    "            self.dataloader[split] = DataLoader(\n",
    "                self.dataset[split],\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collator_obj,\n",
    "            )\n",
    "        \n",
    "    def group_sentences(\n",
    "        self,\n",
    "        dict_data,\n",
    "        max_len=128,\n",
    "        sentence_split_ratio=1.3,\n",
    "    ):\n",
    "        print('grouping sentences...')\n",
    "        list_out = []\n",
    "        cur_sequence = ''\n",
    "        idx = 0\n",
    "        for k_, v_ in tqdm(dict_data.items()):\n",
    "            idx += 1\n",
    "            new_sequence = cur_sequence + v_['TEXT'] + ' '\n",
    "            print((len(new_sequence.split(' ')) * sentence_split_ratio))\n",
    "            if (len(new_sequence.split(' ')) * sentence_split_ratio) >= max_len:\n",
    "                list_out.append(cur_sequence)\n",
    "                cur_sequence = v_['TEXT'] + ' '\n",
    "            else:\n",
    "                cur_sequence = new_sequence\n",
    "            #\n",
    "            if idx > 1000:\n",
    "                break\n",
    "        print('done')\n",
    "        \n",
    "        return list_out\n",
    "    \n",
    "    def split_data(\n",
    "        self,\n",
    "        list_data,\n",
    "        validation_size,\n",
    "        fixed_seed_val,\n",
    "    ):\n",
    "        \n",
    "        #\n",
    "        print('splitting data...')\n",
    "        \n",
    "        #\n",
    "        np.random.seed(fixed_seed_val)\n",
    "        val_indices = np.random.choice(\n",
    "            range(len(list_data)), \n",
    "            size=int(validation_size * len(list_data)),\n",
    "            replace=False,\n",
    "        )\n",
    "        train_indices = list(set(list(range(len(list_data)))) - set(val_indices))\n",
    "        \n",
    "        #\n",
    "        #print((train_indices.__len__(), val_indices.__len__()))\n",
    "        #print(train_indices)\n",
    "        print(list_data.__len__())\n",
    "        #\n",
    "        val_ = np.array(list_data)[val_indices].tolist()\n",
    "        train_ = np.array(list_data)[train_indices].tolist()\n",
    "        \n",
    "        #\n",
    "        assert (len(val_) + len(train_)) == len(list_data)\n",
    "        assert set(val_indices).union(train_indices) == set(list(range(len(list_data))))\n",
    "        \n",
    "        print('done...')\n",
    "        \n",
    "        return train_, val_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77ff2f-2c38-45e9-b844-fef016d0682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "#tokenizer.add_tokens(['<sent-sep>'], special_tokens=True)\n",
    "\n",
    "#\n",
    "d_ = LMDataloader(\n",
    "    dict_data=data,\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    "    max_seq_len=128,\n",
    "    batch_size=8,\n",
    "    validation_size=0.05,\n",
    "    fixed_seed_val=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a234d-2ea2-4247-ac91-a835fc220df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'validation']:\n",
    "    for batch in d_.dataloader[split]:\n",
    "        #print(batch.keys())\n",
    "        print(tokenizer.batch_decode(batch['input_ids'][0]))\n",
    "        for id_ in batch['labels'][0]:\n",
    "            if id_.item() != -100:\n",
    "                print(tokenizer.decode(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf669d-2db8-4c6c-b2a0-d94fe060d680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6deb17-2afc-4e36-8a7a-ba9b2a1ce7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
